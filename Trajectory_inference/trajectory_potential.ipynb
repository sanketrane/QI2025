{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6286e020",
   "metadata": {},
   "source": [
    "# Applying Trajectory Inference Pipeline\n",
    "\n",
    "### Packages you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "plt.style.use('default')\n",
    "\n",
    "from plotly import graph_objects as go\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac499c7",
   "metadata": {},
   "source": [
    "### Adjust your I/O configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.abspath(os.path.join(os.getcwd(), \"..\")) # Adjust the path to your project root directory\n",
    "\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)\n",
    "\n",
    "data_io = os.path.join(root, \"Trajectory_inference\", \"datasets\")\n",
    "data_io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb4975",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Our dataset is derived from **single-cell immune profiling**, where **14 protein markers** were experimentally measured for individual immune cells. Using a **Variational Autoencoder (VAE)**, these high-dimensional measurements are compressed into a **3D latent state space** that captures the underlying structure of the immune system.\n",
    "\n",
    "Guided by **biological domain knowledge**, we hypothesize that one latent dimension — or a derived indicator such as a **GFP**—can serve as a meaningful proxy for the **dynamical progression of immune cells**. Our objective is to perform **trajectory inference** within this latent space to reveal the **hidden dynamical patterns and developmental pathways** that govern immune cell differentiation and state transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550e36f",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e84c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(data_io, \"latent_data_with_GFP.csv\")) # data loading\n",
    "data_sampled = data.sample(n=50000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df36cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cell_type.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bbab9",
   "metadata": {},
   "source": [
    "### Estimate the potential function\n",
    "\n",
    "In this step, we try to describe how cells \"move\" within the 3D latent space. Instead of observing motion directly, we estimate an **energy-like potential function** that explains where cells are more likely to appear and how they might flow between states.\n",
    "\n",
    "Intuitively, you can think of this potential as a **landscape**: cells tend to \"roll down\" from high-potential regions (early states) toward low-potential regions (mature or stable states).  \n",
    "\n",
    "By estimating this potential function, we can visualize and analyze the **direction and strength of cell state transitions**, helping us understand the hidden dynamics of the immune system in a simple, physical way.\n",
    "\n",
    "To estimate the potential function $ P(x) $ from observed single-cell data, we can treat it as a **regression problem** that maps cell coordinates $ x \\in \\mathbb{R}^3 $ to a scalar GFP value $ y \\in [0,1] $:\n",
    "\n",
    "$$\n",
    "y_i \\;=\\; P(x_i) \\;+\\; \\varepsilon_i, \\qquad i = 1, \\dots, N,\n",
    "$$\n",
    "\n",
    "where  \n",
    "- $x_i = (X_i, Y_i, Z_i)$ is the latent coordinate of the $i$-th cell,  \n",
    "- $y_i$ is its observed or estimated GFP level,  \n",
    "- $\\varepsilon_i$ is a noise term capturing measurement or model uncertainty.\n",
    "\n",
    "Our goal is to learn a smooth function $ \\hat{P}(x) $ that minimizes the prediction error:\n",
    "\n",
    "$$\n",
    "\\hat{P} \\;=\\; \\arg\\min_{f \\in \\mathcal{F}} \n",
    "\\sum_{i=1}^{N} \\big( y_i - f(x_i) \\big)^2 ,\n",
    "$$\n",
    "\n",
    "where $ \\mathcal{F} $ can be a family of regression models that are differentiable, such as Polynomial regression, kernel ridge regression, Gaussian process regression, or neural networks.\n",
    "\n",
    "### Calculate the gradient-based vector field\n",
    "\n",
    "Once we have estimated the smooth potential function $\\hat{P}(x)$, we can derive a **vector field** to describe how cells move within the 3D latent space.\n",
    "\n",
    "We interpret $\\hat{P}(x)$ as an energy-like potential landscape. Cells tend to move “downhill” from regions of high potential (early or unstable states) to regions of low potential (mature or stable states). The corresponding velocity field is defined by the gradient:\n",
    "\n",
    "$$\n",
    "v(x) = \\nabla \\hat{P}(x)\n",
    "$$\n",
    "\n",
    "To stabilize and normalize the flow magnitude, we apply a smooth scaling factor:\n",
    "\n",
    "$$\n",
    "v(x) = \\frac{\\nabla \\hat{P}(x)}{\\lambda + \\| \\nabla \\hat{P}(x) \\| + \\varepsilon}\n",
    "$$\n",
    "\n",
    "where  \n",
    "- $\\lambda > 0$ controls the decay rate (flow scale),  \n",
    "- $\\varepsilon > 0$ prevents division by zero,  \n",
    "- and the direction of $v(x)$ always points toward decreasing potential.\n",
    "\n",
    "This gradient-based vector field $v(x)$ encodes the **direction and speed** of cell state transitions.  \n",
    "It allows us to visualize the inferred dynamics, identify attractors (low-potential basins), and trace developmental trajectories in the latent space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342f7ad",
   "metadata": {},
   "source": [
    "#### Here is the pre-defined pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnPolynomialScalarRegressor:\n",
    "    \"\"\"\n",
    "    Sklearn-based scalar regressor g(z) with a torch-diffable forward.\n",
    "\n",
    "    - Fit:    g(z) ≈ w^T * Poly(z) + b  using PolynomialFeatures + LinearRegression\n",
    "    - Use:    g_np = reg(X)            # numpy, shape (N,)\n",
    "              g_torch = reg.torch_forward(z)  # torch tensor, shape (N,)\n",
    "\n",
    "    Designed to match an 'approximate_with_polynomial' style setup,\n",
    "    and to be lightweight for use inside a single notebook.\n",
    "    \"\"\"\n",
    "    def __init__(self, degree: int = 2, include_bias: bool = True):\n",
    "        self.degree = int(degree)\n",
    "        self.include_bias = bool(include_bias)\n",
    "\n",
    "        self.poly = None          # type: PolynomialFeatures | None\n",
    "        self.lin = None           # type: LinearRegression | None\n",
    "\n",
    "        # Cached params for fast / torch-friendly evaluation\n",
    "        self.powers_ = None       # (K, d)\n",
    "        self.coef_ = None         # (K,)\n",
    "        self.intercept_ = 0.0\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y, dtype=np.float64).reshape(-1)\n",
    "\n",
    "        self.poly = PolynomialFeatures(\n",
    "            degree=self.degree,\n",
    "            include_bias=self.include_bias\n",
    "        )\n",
    "        X_poly = self.poly.fit_transform(X)\n",
    "\n",
    "        self.lin = LinearRegression()\n",
    "        self.lin.fit(X_poly, y)\n",
    "\n",
    "        # Cache polynomial structure for manual / torch evaluation\n",
    "        self.powers_ = self.poly.powers_.astype(np.int64)   # (K, d)\n",
    "        self.coef_ = self.lin.coef_.astype(np.float64)      # (K,)\n",
    "        self.intercept_ = float(self.lin.intercept_)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Call fit() before predict().\")\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        if X.ndim == 1:\n",
    "            X = X[None, :]\n",
    "        # Use transform + linear model in sklearn, stable and reliable\n",
    "        X_poly = self.poly.transform(X)\n",
    "        return self.lin.predict(X_poly).reshape(-1)\n",
    "\n",
    "    def __call__(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.predict(X)\n",
    "\n",
    "    def torch_forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Differentiable forward: g(z) = sum_k coef[k] * prod_j z_j ** powers[k, j] + intercept\n",
    "\n",
    "        z: (N, d) float tensor on any device\n",
    "        return: (N,) float tensor\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model not fitted.\")\n",
    "        if z.ndim == 1:\n",
    "            z = z.unsqueeze(0)\n",
    "\n",
    "        # Prepare parameters on same device/dtype\n",
    "        powers = torch.as_tensor(self.powers_, dtype=z.dtype, device=z.device)   # (K, d)\n",
    "        coef = torch.as_tensor(self.coef_, dtype=z.dtype, device=z.device)       # (K,)\n",
    "        intercept = torch.as_tensor(self.intercept_, dtype=z.dtype, device=z.device)\n",
    "\n",
    "        # z_expanded: (N, 1, d), powers: (1, K, d)\n",
    "        # terms[n, k, j] = z[n, j] ** powers[k, j]\n",
    "        z_expanded = z.unsqueeze(1)\n",
    "        powers_expanded = powers.unsqueeze(0)\n",
    "        terms = z_expanded ** powers_expanded      # (N, K, d)\n",
    "\n",
    "        # product over variables j → (N, K)\n",
    "        feats = terms.prod(dim=-1)\n",
    "\n",
    "        # linear combination → (N,)\n",
    "        g = (feats * coef.unsqueeze(0)).sum(dim=1) + intercept\n",
    "        return g\n",
    "\n",
    "\n",
    "class PotentialFieldFromScalar:\n",
    "    \"\"\"\n",
    "    Turn any scalar regressor g(z) into a vector field via gradient-based construction.\n",
    "\n",
    "    Default (GFP-inspired) mapping:\n",
    "\n",
    "        base(z) = -∇g(z) / (lambda * g(z) + eps)\n",
    "\n",
    "    If normalize=True:\n",
    "\n",
    "        v(z) = base(z) / ( ||base(z)||^2 + eps )\n",
    "\n",
    "    Otherwise:\n",
    "\n",
    "        v(z) = base(z)\n",
    "\n",
    "    Uses torch.autograd on the provided scalar_regressor.torch_forward.\n",
    "    Suitable for use directly in a notebook.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        scalar_regressor: SklearnPolynomialScalarRegressor,\n",
    "        decay_rate: float = np.log(2) / 3.0,\n",
    "        eps: float = 1e-8,\n",
    "        normalize: bool = True,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.scalar = scalar_regressor\n",
    "        self.lambda_decay = float(decay_rate)\n",
    "        self.eps = float(eps)\n",
    "        self.normalize = bool(normalize)\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "    def fit(self, X: np.ndarray, V: np.ndarray | None = None):\n",
    "        \"\"\"\n",
    "        Placeholder for API compatibility.\n",
    "        Typically, fit the scalar regressor separately, then wrap it here.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def __call__(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute v(z) on a batch:\n",
    "            X: (N, d) numpy array\n",
    "            return: (N, d) numpy array\n",
    "        \"\"\"\n",
    "        if not self.scalar.is_fitted:\n",
    "            raise RuntimeError(\"Scalar regressor must be fitted before using PotentialFieldFromScalar.\")\n",
    "\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        if X.ndim == 1:\n",
    "            X = X[None, :]\n",
    "\n",
    "        z = torch.tensor(X, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "\n",
    "        # g(z): (N,)\n",
    "        g = self.scalar.torch_forward(z)\n",
    "\n",
    "        # ∇g(z): (N, d)\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=g,\n",
    "            inputs=z,\n",
    "            grad_outputs=torch.ones_like(g),\n",
    "            create_graph=False,\n",
    "            retain_graph=False,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "\n",
    "        # base(z) = - ∇g / (lambda * g + eps)\n",
    "        denom = self.lambda_decay * g.unsqueeze(1) + self.eps\n",
    "        base = - grad / denom\n",
    "\n",
    "        if self.normalize:\n",
    "            # v(z) = base / (||base||^2 + eps)\n",
    "            norm_sq = (base ** 2).sum(dim=1, keepdim=True) + self.eps\n",
    "            v = base / norm_sq\n",
    "        else:\n",
    "            v = base\n",
    "\n",
    "        # Convert to numpy - use np.array() to avoid torch-numpy ABI mismatch\n",
    "        v_detached = v.detach().cpu()\n",
    "        return np.array(v_detached)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d689168",
   "metadata": {},
   "source": [
    "#### Apply the pre-defined pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a2bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "approximate_with_poly = SklearnPolynomialScalarRegressor(degree=7).fit(data[[\"X\", \"Y\", \"Z\"]].to_numpy(), data[\"GFP\"].to_numpy())\n",
    "vec_fn = PotentialFieldFromScalar(approximate_with_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478a57c",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_arrows = min(80000, len(data))\n",
    "\n",
    "points_with_gfp = data.sample(n=N_arrows, random_state=0)[[\"X\", \"Y\", \"Z\", \"GFP\"]].to_numpy()\n",
    "\n",
    "# label encoding for cell types\n",
    "label_encoder = LabelEncoder()\n",
    "data['cell_type_encoded'] = label_encoder.fit_transform(data['cell_type'])\n",
    "points_with_cluster = data.sample(n=N_arrows, random_state=0)[[\"X\", \"Y\", \"Z\", \"cell_type_encoded\"]].to_numpy()\n",
    "points = data.sample(n=N_arrows, random_state=0)[[\"X\", \"Y\", \"Z\"]].to_numpy()\n",
    "\n",
    "V = vec_fn(points)  # Expected shape: (N_arrows, 3)\n",
    "\n",
    "v_norm = np.linalg.norm(V, axis=1)\n",
    "# Clip extreme vectors to avoid a “spiky” appearance\n",
    "clip_thr = np.percentile(v_norm, 95)\n",
    "mask = v_norm > 0\n",
    "V_clipped = np.where(\n",
    "    (v_norm[:, None] > 0) & (v_norm[:, None] < clip_thr),\n",
    "    V,\n",
    "    V * (clip_thr / (v_norm[:, None] + 1e-9))\n",
    ")\n",
    "v_norm = np.linalg.norm(V_clipped, axis=1) + 1e-9  # avoid div by zero\n",
    "\n",
    "# Normalize directions and scale arrow length\n",
    "V_dir = V_clipped / v_norm[:, None]\n",
    "length_scale = 0.2\n",
    "V_plot = V_dir * length_scale\n",
    "\n",
    "# Color arrows by speed magnitude\n",
    "normed = (v_norm - v_norm.min()) / (v_norm.max() - v_norm.min() + 1e-12)\n",
    "colors = plt.cm.viridis(normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8367129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with cell type clustering\n",
    "# Using V_plot and points from previous code cell\n",
    "data_sampled = pd.DataFrame({\n",
    "    'X': points[:, 0],\n",
    "    'Y': points[:, 1],\n",
    "    'Z': points[:, 2],\n",
    "    'Vx': V_plot[:, 0],\n",
    "    'Vy': V_plot[:, 1],\n",
    "    'Vz': V_plot[:, 2],\n",
    "    'speed': np.linalg.norm(V_clipped, axis=1),\n",
    "    'cell_type': points_with_cluster[:, 3]\n",
    "})\n",
    "\n",
    "# Inverse transform to original cell type labels\n",
    "data_sampled['cell_type'] = label_encoder.inverse_transform(data_sampled['cell_type'].astype(int).to_numpy())\n",
    "\n",
    "# no FM B Cells go to MZ\n",
    "# plot quiver plot using plotly\n",
    "fig = px.scatter_3d(\n",
    "    data_sampled,\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    z='Z',\n",
    "    color='cell_type',\n",
    "    color_discrete_map= {\n",
    "        'T1': 'red',\n",
    "        'T2': 'orange',\n",
    "        'FM': 'green',\n",
    "        'MZP': 'blue'\n",
    "    },\n",
    "    title='3D Velocity Field Derived from Polynomial Potential',\n",
    "    range_x=[-0.5, 0.5],\n",
    "    range_y=[-0.5, 0.5],\n",
    "    range_z=[-0.5, 0.5],\n",
    "    opacity=0.1,\n",
    "    size='speed',\n",
    "    size_max=10\n",
    ")\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "\n",
    "max_arrows = 1000  \n",
    "if len(data_sampled) > max_arrows:\n",
    "    data_arrow = data_sampled.sample(max_arrows, random_state=0)\n",
    "else:\n",
    "    data_arrow = data_sampled\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Cone(\n",
    "        x=data_arrow['X'],\n",
    "        y=data_arrow['Y'],\n",
    "        z=data_arrow['Z'],\n",
    "        u=data_arrow['Vx'],\n",
    "        v=data_arrow['Vy'],\n",
    "        w=data_arrow['Vz'],\n",
    "        anchor=\"tail\",               \n",
    "        sizemode=\"absolute\",\n",
    "        sizeref=0.5,             \n",
    "        colorscale=\"blues\",\n",
    "        cmin=data_sampled['speed'].min(),\n",
    "        cmax=data_sampled['speed'].max(),\n",
    "        showscale=False               \n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        aspectmode=\"data\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#save fig\n",
    "fig.write_html(os.path.join(root, \"Trajectory_inference\", \"results\", \"gfp_potential_trajectory.html\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
