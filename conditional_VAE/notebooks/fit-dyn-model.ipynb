{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a631f4",
   "metadata": {},
   "source": [
    "# Fit a dynamical model to sc-flow data\n",
    "\n",
    "In this notebook, we will\n",
    "\n",
    "1. Import and visualize simulated single cell flow cytometry data\n",
    "2. Develop a conditional variational autoencoder that finds a latent space representation, and estimates differentiation rates\n",
    "3. Visualize the results and compare the inferred values with the ground truth\n",
    "4. Restrict the model space and improve our estimates\n",
    "\n",
    "We start by importing some python modules. We will use `torch` and `pyro` for deep learing and variational inference. This is faster on a GPU, but can also be done on a CPU if required. We therefore check if a GPU is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb689c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TABLEAU_COLORS\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from scdynsys.dynamic_vae import VAEgmmdyn\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "print(\"CUDA available:\", USE_CUDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fb3b3",
   "metadata": {},
   "source": [
    "## 1. Import the data and visualize\n",
    "\n",
    "The dataset contains values for 12 flow cytometry markers, and time and celltype information for each cell. The celltype is typically not known, but as this data is simulated, we actually know the celltype. Of course, we should not use this information in our model fitting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b92672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the (simulated) dataset\n",
    "dataset = pd.read_csv('../data/simulated_time_series_data.csv')\n",
    "\n",
    "# look at the first few rows\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a274dbbd",
   "metadata": {},
   "source": [
    "### Inspection of the data\n",
    "\n",
    "The dataset contains: \n",
    "\n",
    "- 12 flow cytometry markers \n",
    "- sampling time information\n",
    "\n",
    "Lets plot 1-d flow plots (marginal densities) \n",
    "and see how these change with time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6483711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot marginal densities of markers at different time points\n",
    "time_points = sorted(dataset['Timepoint'].unique())\n",
    "num_timepoints = len(time_points)\n",
    "markers = [col for col in dataset.columns if col not in ['Timepoint', 'CellType']]\n",
    "num_markers = len(markers)\n",
    "fig, axs = plt.subplots(\n",
    "    num_timepoints, num_markers,\n",
    "    figsize=(2*num_markers, 2*num_timepoints), \n",
    "    sharex='col', sharey=True\n",
    ")\n",
    "\n",
    "fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "\n",
    "for j, marker in enumerate(markers):\n",
    "    for i, tp in enumerate(time_points):\n",
    "        data_tp = dataset[dataset['Timepoint'] == tp][marker]\n",
    "        ax = axs[i,j]\n",
    "        ax.hist(data_tp, bins=30, density=True, color='k', alpha=0.7)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f'Time {tp}')\n",
    "        if i == 0:\n",
    "            ax.set_title(marker)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e476199",
   "metadata": {},
   "source": [
    "The dataset is quite large, and so we'll subsample to speed up\n",
    "computations. \n",
    "Next, we'll define some arrays and tensors that are required below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d82b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sample(50000, random_state=42)\n",
    "\n",
    "xs_raw = dataset.iloc[:, 0:-1].values\n",
    "times_raw = dataset[\"Timepoint\"].values\n",
    "ts_tensor = torch.tensor(times_raw, dtype=torch.float32, device=DEVICE)\n",
    "celltypes = dataset[\"CellType\"].values\n",
    "celltypes_unique = np.unique(celltypes)\n",
    "print(f\"Unique cell types: {celltypes_unique}\")\n",
    "\n",
    "\n",
    "xs_tensor = torch.tensor(xs_raw, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "data_dim = xs_tensor.shape[1]  # number of features\n",
    "n_samples = xs_tensor.shape[0]\n",
    "\n",
    "print(f\"Number of features: {data_dim}, Number of samples: {n_samples}\")\n",
    "\n",
    "# create unique time tensor\n",
    "\n",
    "utime_raw = np.sort(dataset[\"Timepoint\"].unique())\n",
    "utime_tensor = torch.tensor(utime_raw, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "num_timepoints = len(utime_raw)\n",
    "print(f\"Number of unique time points: {num_timepoints}\")\n",
    "\n",
    "print(f\"Unique time points: {utime_raw}\")\n",
    "\n",
    "# create time index tensor\n",
    "\n",
    "xtime_raw = np.sum([i * (times_raw == t) for i, t in enumerate(utime_raw)], axis=0)\n",
    "xtime_tensor = torch.tensor(xtime_raw, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "# check that the time index tensor is correct\n",
    "\n",
    "assert torch.all(utime_tensor[xtime_tensor] == ts_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5301e",
   "metadata": {},
   "source": [
    "## 2. Develop a conditional VAE\n",
    "\n",
    "TODO: text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAEgmmdyn(\n",
    "    data_dim = data_dim, \n",
    "    z_dim = 3, \n",
    "    hidden_dim = 24,\n",
    "    num_clus = 4,\n",
    "    use_cuda = USE_CUDA,\n",
    ")\n",
    "\n",
    "# print the model architecture:\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def train_loop_full_dataset(\n",
    "    svi: SVI,\n",
    "    xs_raw: np.array,\n",
    "    xtime_raw: np.array,\n",
    "    utime_raw: np.array,\n",
    "    num_epochs: int\n",
    ") -> list[tuple[int, float]]:\n",
    "    trange = tqdm.notebook.trange\n",
    "    \n",
    "    xs_tensor = torch.tensor(xs_raw, device=DEVICE, dtype=torch.float32)\n",
    "    xtime_tensor = torch.tensor(xtime_raw, device=DEVICE, dtype=torch.long)\n",
    "    utime_tensor = torch.tensor(utime_raw, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    N = torch.tensor(xs_tensor.shape[0], device=DEVICE)\n",
    "    \n",
    "    train_elbo = []\n",
    "            \n",
    "    for epoch in (pbar := trange(num_epochs)):    \n",
    "        total_epoch_loss_train = svi.step(xs_tensor, xtime_tensor, utime_tensor, N)\n",
    "        \n",
    "        train_elbo.append((epoch, total_epoch_loss_train))\n",
    "        pbar.set_description(f\"train loss: {total_epoch_loss_train:0.2f}\")\n",
    "\n",
    "    return train_elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aadb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model \n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": 2e-3}\n",
    "optimizer = Adam(adam_args)\n",
    "NUM_EPOCHS = 5000\n",
    "\n",
    "# setup the inference algorithm\n",
    "loss_method = Trace_ELBO(num_particles=10, vectorize_particles=True)\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=loss_method)\n",
    "\n",
    "# training loop\n",
    "\n",
    "train_elbo = train_loop_full_dataset(\n",
    "    svi, \n",
    "    xs_raw,\n",
    "    xtime_raw,\n",
    "    utime_raw,\n",
    "    NUM_EPOCHS\n",
    ")\n",
    "\n",
    "# plot the training loss curve\n",
    "train_elbo_vals = [x[1] for x in train_elbo]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "ax.plot(train_elbo_vals, linewidth=0.5, color='k')\n",
    "ax.set_title(\"Training loss curve\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"ELBO Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16eb87a",
   "metadata": {},
   "source": [
    "## 3. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## infer cell types \n",
    "\n",
    "with torch.no_grad():\n",
    "    zs_tensor = vae.dimension_reduction(xs_tensor, ts_tensor).cpu()\n",
    "    zs_raw = zs_tensor.numpy()\n",
    "\n",
    "# do PCA on the latent space\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(zs_raw)\n",
    "zs_pca = pca.transform(zs_raw)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    clus = vae.classifier(xs_tensor, ts_tensor, method='sample')\n",
    "clus = clus.cpu().numpy()\n",
    "\n",
    "# find the ground truth cell types that are most represented in each inferred cluster\n",
    "# make a dictionary to translate cluster indices to cell type names\n",
    "translation_dict = {}\n",
    "celltypes = dataset[\"CellType\"].values\n",
    "celltypes_unique = sorted(dataset[\"CellType\"].unique()) \n",
    "for clus_idx in range(vae.num_clus):\n",
    "    celltype_counts = {}\n",
    "    for ct in celltypes_unique:\n",
    "        celltype_counts[ct] = np.sum((clus == clus_idx) & (celltypes == ct))\n",
    "    most_common_celltype = max(celltype_counts, key=celltype_counts.get)\n",
    "    print(f\"Cluster {clus_idx}: most common cell type: {most_common_celltype} ({celltype_counts[most_common_celltype]} cells)\")\n",
    "    translation_dict[clus_idx] = most_common_celltype\n",
    "# map inferred cluster indices to cell type names\n",
    "clus_names = [translation_dict[clus_idx] for clus_idx in clus]\n",
    "\n",
    "# define a permutation of cluster indices based on alphabetical order of cell type names\n",
    "permutation = sorted(translation_dict.keys(), key=lambda k: translation_dict[k])\n",
    "\n",
    "# re-index clusters to have consistent colors\n",
    "new_clus = np.zeros_like(clus)\n",
    "for new_idx, clus_idx in enumerate(permutation):\n",
    "    new_clus[clus == clus_idx] = new_idx\n",
    "clus = new_clus\n",
    "\n",
    "\n",
    "# visualize inferred cell types in PCA space\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(6, 6))\n",
    "\n",
    "cols = list(TABLEAU_COLORS.values())\n",
    "\n",
    "for clus_idx in range(vae.num_clus):\n",
    "    ax = axs[0, 0]\n",
    "    ax.scatter(zs_pca[clus == clus_idx, 0], zs_pca[clus == clus_idx, 1], s=1,\n",
    "               linewidths=0, c=cols[clus_idx])\n",
    "    ax.set_title('PCA 1 vs PCA 2')\n",
    "    ax = axs[0, 1]\n",
    "    ax.scatter(zs_pca[clus == clus_idx, 0], zs_pca[clus == clus_idx, 2], s=1,\n",
    "                linewidths=0, c=cols[clus_idx])\n",
    "    ax.set_title('PCA 1 vs PCA 3')\n",
    "    ax = axs[1, 0]\n",
    "    ax.scatter(zs_pca[clus == clus_idx, 2], zs_pca[clus == clus_idx, 1], s=1,\n",
    "                linewidths=0, c=cols[clus_idx])\n",
    "    ax.set_title('PCA 3 vs PCA 2')\n",
    "\n",
    "axs[1, 1].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample Q values from the fitted model\n",
    "\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(\n",
    "    vae.model, guide=vae.guide, num_samples=100, \n",
    "    return_sites=[\"X0\", \"Qoffdiag\"],\n",
    "    parallel=True\n",
    ")\n",
    "\n",
    "samples = predictive(\n",
    "    xs_tensor, \n",
    "    xtime_tensor, \n",
    "    utime_tensor, \n",
    "    N=torch.tensor(n_samples, device=DEVICE)\n",
    ")\n",
    "\n",
    "Qoffdiag_samples = samples[\"Qoffdiag\"].cpu().numpy().squeeze()\n",
    "weights_samples = samples[\"X0\"].cpu().numpy().squeeze()\n",
    "\n",
    "## apply the permutation to the Q matrices and weights\n",
    "\n",
    "weights_samples_permuted = weights_samples[:, permutation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc9c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "violins = ax.violinplot(weights_samples_permuted, showmeans=False, showextrema=False)\n",
    "for pc in violins['bodies']:\n",
    "    pc.set_facecolor('tab:blue')\n",
    "    pc.set_alpha(1)\n",
    "\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.set_title(\"GMM Cluster Weights Distribution\")\n",
    "\n",
    "# compare with true weights at time 0\n",
    "\n",
    "cts_t0 = dataset[dataset['Timepoint'] == utime_raw[0]]['CellType'].values\n",
    "\n",
    "true_weights = [\n",
    "    np.sum(cts_t0 == ct) / len(cts_t0) \n",
    "    for ct in sorted(celltypes_unique)\n",
    "]\n",
    "\n",
    "ax.scatter(np.arange(1, vae.num_clus + 1), true_weights, color='red', label='True Weights', zorder=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a5b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scdynsys.dynamic_model import build_Q_mat\n",
    "\n",
    "# import ground truth Q matrix\n",
    "\n",
    "Q_gt_df = pd.read_csv('../data/simulated_time_series_Q_matrix.csv', index_col=0)\n",
    "\n",
    "Q_gt = Q_gt_df.values\n",
    "\n",
    "Q_gt_zero_diag = Q_gt.copy()\n",
    "np.fill_diagonal(Q_gt_zero_diag, 0)\n",
    "\n",
    "# plot Q\n",
    "\n",
    "Qoffdiag_samples = samples[\"Qoffdiag\"].cpu().squeeze()\n",
    "Q_samples = build_Q_mat(Qoffdiag_samples).numpy()\n",
    "\n",
    "# apply permutation to Q matrices\n",
    "\n",
    "Q_samples_permuted = Q_samples[:, permutation, :][:, :, permutation]\n",
    "\n",
    "meanQ = np.mean(Q_samples_permuted, axis=0)\n",
    "# set diagonal entries to zero \n",
    "np.fill_diagonal(meanQ, 0)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "ax = axs[0]\n",
    "im = ax.pcolor(meanQ, cmap='viridis', vmin=0, vmax=np.max(Q_gt))\n",
    "fig.colorbar(im, ax=ax)\n",
    "\n",
    "# plot the ground true Q matrix for comparison\n",
    "ax = axs[1]\n",
    "\n",
    "im = ax.pcolor(Q_gt_zero_diag, cmap='viridis')\n",
    "fig.colorbar(im, ax=ax)\n",
    "\n",
    "axs[0].set_title(\"Inferred Q matrix\")\n",
    "axs[1].set_title(\"Ground Truth Q matrix\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"From Cluster\")\n",
    "    ax.set_ylabel(\"To Cluster\")\n",
    "    ticks = np.arange(0.5, vae.num_clus + 0.5)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_xticklabels(celltypes_unique)\n",
    "    ax.set_yticklabels(celltypes_unique)\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f5bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot latents space embeddings for each time points\n",
    "\n",
    "with torch.no_grad():\n",
    "    zs_tensor = vae.dimension_reduction(xs_tensor, ts_tensor).cpu()\n",
    "    zs_raw = zs_tensor.numpy()\n",
    "\n",
    "# do PCA on the latent space\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(zs_raw)\n",
    "zs_pca = pca.transform(zs_raw)\n",
    "\n",
    "# plot PCA 1 and 2 in different panels as a function of time\n",
    "\n",
    "fig, axs = plt.subplots(3, num_timepoints, figsize=(3*num_timepoints, 9), \n",
    "                        sharey=True, sharex=True)\n",
    "\n",
    "for t_idx in range(num_timepoints):\n",
    "    ax1, ax2, ax3 = axs[:, t_idx]\n",
    "    time_mask = (xtime_raw == t_idx)\n",
    "    ax1.scatter(zs_pca[time_mask, 0], zs_pca[time_mask, 1], s=1, alpha=1,\n",
    "                   linewidths=0, c='k')\n",
    "    ax2.scatter(zs_pca[time_mask, 0], zs_pca[time_mask, 2], s=1, alpha=1,\n",
    "                   linewidths=0, c='k')\n",
    "    ax3.scatter(zs_pca[time_mask, 2], zs_pca[time_mask, 1], s=1, alpha=1,\n",
    "                   linewidths=0, c='k')\n",
    "    ax1.set_title(f'Time {utime_raw[t_idx]}')\n",
    "    ax1.set(xlabel='PCA 1', ylabel='PCA 2')\n",
    "    ax2.set(xlabel='PCA 1', ylabel='PCA 3')\n",
    "    ax3.set(xlabel='PCA 3', ylabel='PCA 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f529cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import ground truth weights for comparison\n",
    "\n",
    "ground_truth_weights = pd.read_csv('../data/simulated_time_series_ground_truth_weights.csv')\n",
    "\n",
    "## simulate trajectories from the fitted model\n",
    "\n",
    "ws_samples = vae.sample_trajectories(\n",
    "    n=100,\n",
    "    ts=utime_tensor,    \n",
    ")\n",
    "\n",
    "ws_samples_raw = ws_samples.cpu().numpy()\n",
    "\n",
    "## apply permutation to the weights\n",
    "ws_samples_raw = ws_samples_raw[:, :, permutation]\n",
    "\n",
    "mean_ws = np.mean(ws_samples_raw, axis=0)\n",
    "lower_ws = np.percentile(ws_samples_raw, 5, axis=0)\n",
    "upper_ws = np.percentile(ws_samples_raw, 95, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for i in range(vae.num_clus):\n",
    "    ax.plot(utime_raw, mean_ws[:, i], label=f\"Cluster {i+1}\", color=list(TABLEAU_COLORS.values())[i])\n",
    "    ax.fill_between(utime_raw, lower_ws[:, i], upper_ws[:, i], color=list(TABLEAU_COLORS.values())[i], alpha=0.3)\n",
    "\n",
    "    ax.scatter(ground_truth_weights['Timepoint'], ground_truth_weights[f'Cluster_{i}_Weight'],\n",
    "               color=list(TABLEAU_COLORS.values())[i], marker='x')\n",
    "    \n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Cluster Weights\")\n",
    "ax.set_title(\"Inferred GMM Cluster Weights over Time\")\n",
    "\n",
    "# add ground truth legend\n",
    "ax.scatter([], [], color='k', marker='x', label='Ground Truth')\n",
    "\n",
    "ax.legend(fontsize='small', loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a1949",
   "metadata": {},
   "source": [
    "### Show differentiation pathways between clusters\n",
    "\n",
    "Next, we will give a visual representation of how cells develop. For this, we will plot the cluster centers (locations) in the latent space (after applying PCA),\n",
    "together with arrows indicating differentiation between the cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a diagram with differentation rates \n",
    "\n",
    "loc_pred = Predictive(vae.mix, guide=vae.mix.guide,\n",
    "                      num_samples=100, parallel=True, return_sites=[\"clus_locs\"])\n",
    "loc_samples = loc_pred()[\"clus_locs\"].cpu().numpy()\n",
    "\n",
    "# apply the PCA transformation\n",
    "\n",
    "locs_pca = pca.transform(loc_samples.reshape(-1, vae.z_dim))\n",
    "locs_pca = locs_pca.reshape(-1, vae.num_clus, vae.z_dim)\n",
    "\n",
    "# plot the locs in PCA space\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# plot the estimated arrows between the cluster centers in PCA space\n",
    "ax = axs[0]\n",
    "\n",
    "def plot_locs(ax, locs_pca):\n",
    "    for i in range(vae.num_clus):\n",
    "        ax.scatter(locs_pca[:, i, 0], locs_pca[:, i, 1], s=5, alpha=0.5,\n",
    "                color=list(TABLEAU_COLORS.values())[i], label=celltypes_unique[i])\n",
    "        \n",
    "    ax.set_xlabel(\"PCA 1\")\n",
    "    ax.set_ylabel(\"PCA 2\")\n",
    "    ax.legend(fontsize='small')\n",
    "\n",
    "plot_locs(axs[0], locs_pca)\n",
    "\n",
    "# plot arrows between the locs according to the Q matrix\n",
    "Q_mean = Q_samples_permuted.mean(axis=0)\n",
    "\n",
    "# make sure that the width of the arrows is proportional to the Q values\n",
    "\n",
    "def plot_arrows(ax, Q_mean, locs_pca):\n",
    "    for i in range(vae.num_clus):\n",
    "        for j in range(vae.num_clus):\n",
    "            if i != j and Q_mean[i, j] > 0.05:  # threshold for visibility\n",
    "                start = np.mean(locs_pca[:, i, 0]), np.mean(locs_pca[:, i, 1])\n",
    "                end = np.mean(locs_pca[:, j, 0]), np.mean(locs_pca[:, j, 1])\n",
    "                ax.annotate(\"\",\n",
    "                            xy=start, xycoords='data',\n",
    "                            xytext=end, textcoords='data',\n",
    "                            arrowprops=dict(arrowstyle=\"->\", color='k', lw=Q_mean[i, j]*5,\n",
    "                                            shrinkA=10, shrinkB=10,connectionstyle=\"arc3,rad=0.1\"),\n",
    "                            )\n",
    "\n",
    "plot_arrows(axs[0], Q_mean, locs_pca)\n",
    "\n",
    "# now plot the ground truth arrows\n",
    "\n",
    "plot_locs(axs[1], locs_pca)\n",
    "plot_arrows(axs[1], Q_gt, locs_pca)\n",
    "\n",
    "axs[0].set_title(\"inferred differentiation rates\")\n",
    "axs[1].set_title(\"ground truth differentiation rates\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f942253",
   "metadata": {},
   "source": [
    "## 4. Restrict possible differentiation pathways. \n",
    "\n",
    "As you can see in the above diagram, The model predicts differentiation pathways that we did not put into the simulated data. Our fitted model allows for $4\\times 3 = 12$ possible differentiation pairs (even loops). \n",
    "\n",
    "This freedom might not be biologically plausible, and maybe you have some information to restrict some of these pathways. The goal is to put such restrictions in the model. \n",
    "\n",
    "The simplest way to do this is to multiply the matrix $Q$ with a binary \"mask\" matrix $M$, with a 1 indicating that a path is allowed and a 0 that it is not feasible. Note that the multiplication of $Q$ and $M$ is elementwise.\n",
    "\n",
    "1. Modify the dynamical model to accept a mask matrix $M$.\n",
    "2. Ass the mask $M$ to the initialization function of the VAE, and make sure it finds it's way to the right functions.\n",
    "3. Figure out which index pair of $M$ correspond with a cluster index in the model. What happens if you reset parameters and fit the restricted model directly?\n",
    "4. Experiment with first fitting the unrestricted model ($M_{ij} = 1$), and then re-fitting with a different $M$. \n",
    "5. Compare different resticted models in terms of ELBO and goodness of fit (visually). Can you find/select the best fitting model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ea3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b9f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
