{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b3577e",
   "metadata": {},
   "source": [
    "# Pyro 101 - *A quick introduction to Pyro*\n",
    "\n",
    "This notebooks shows the core principles of [Pyro](https://pyro.ai/)\n",
    "Which is a convenient framework for variational inference. \n",
    "\n",
    "In this notebook, we will\n",
    "\n",
    "1. simulate a very simple dataset, using a linear model\n",
    "2. Introduce Pyro \"ingredients\" required to define and fit a model to the data.\n",
    "3. Use stochastic variational inference to fit our model\n",
    "4. observe where how our variational model is mis-specified, and find a way to fix it\n",
    "5. (optional) build your own model and data, and use Pyro to fit model to data yourself.\n",
    "\n",
    "\n",
    "Without going too much in details, you can see variational inference as a method for recovering a family of possible models that can explain your data, and for each member of the model family, we get an approximation of how good this element is.\n",
    "This approximation is given by the so-called variational distribution, which is called a **guide** in Pyro (the model is simply called **model**).\n",
    "\n",
    "More specifically, a Pyro model specifies a **prior distribution** of the model's parameters: this is an way of encoding which parameter values are more likely than others, based on your knowledge before you see the data. \n",
    "\n",
    "The other essential part of a Pyro model is the **data distribution**: this specifies the likelihood of a data point, given a specific set of parameters.\n",
    "\n",
    "**Let's look at a very simple example to see what this means in practice.**\n",
    "\n",
    "First, we have to import some python modules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40693b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import pyro\n",
    "\n",
    "import pyro\n",
    "import torch\n",
    "\n",
    "# then import pyro.distributions\n",
    "import pyro.distributions as dist\n",
    "\n",
    "# import the SVI class for stochastic variational inference\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "# import the Adam optimizer\n",
    "from pyro.optim import Adam\n",
    "\n",
    "# we also need some plotting tools and other stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6467e67",
   "metadata": {},
   "source": [
    "## 1. Create a dataset\n",
    "\n",
    "Let's generate some simple data\n",
    "\n",
    "We'll simulate pairs $(x,y)$ using a linear model. We'll usume that the measurement error $\\sigma$ is known and fix it to 0.5. Notice that the $x$ values are not centered at zero. This will be important later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.5 # assumed known\n",
    "xs = torch.linspace(-1, 5, 100)\n",
    "# ground truth slope and intercept are 0.5 and 1\n",
    "ys = 0.5 * xs + 1 + torch.randn(xs.size()) * sigma\n",
    "# stack the data into pairs. The first dimension is the \"batch dimension\"\n",
    "# the second dimension is the \"feature dimension\"\n",
    "data = torch.stack([xs, ys], dim=-1)\n",
    "\n",
    "# plot the data\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "ax.scatter(xs.numpy(), ys.numpy(), color='k')\n",
    "ax.set(xlabel='x', ylabel='y', title=\"simulated data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a806bb",
   "metadata": {},
   "source": [
    "## 2. Pyro ingredients\n",
    "\n",
    "Next, we'll define a linear model in Pyro.\n",
    "\n",
    "A Pyro model is a python function that usually takes data as input, and contains a number of so-called Pyro *primitives*. These include \n",
    "\n",
    "- `pyro.sample`: for sampling parameters from a (prior) distribution. You'll have to give the name (a string) of the parameter, and the (prior) distribution. `pyro.sample` is also used to \"sample\" data, or to compute the likelihood of some observed value.\n",
    "- `pyro.plate`: this is a way to deal with multiple observations and signals that individual observations are conditionally independent.\n",
    "\n",
    "In our model, we will first sample a slope and intercept from some prior distribution (we choose a standard normal). Then, we will create the expected value of $y$ (called `yhat` below): $$\\hat{y} = \\text{slope} \\times x + \\text{intercept}$$ \n",
    "Finally, we use again the `pyro.sample` primitive to \"sample\" the data, but this time we also include the observed value $y$: this will be used to compute the likelihood of $y$ given $\\hat{y}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # prior distributions for slope and intercept\n",
    "    slope = pyro.sample(\"slope\", dist.Normal(0., 1.))\n",
    "    intercept = pyro.sample(\"intercept\", dist.Normal(0., 1.))\n",
    "    \n",
    "    # likelihood: how the data is generated given slope and intercept\n",
    "    with pyro.plate(\"data\", data.size(0)):\n",
    "        x = data[:, 0]\n",
    "        y = data[:, 1]\n",
    "        yhat = slope * x + intercept\n",
    "        pyro.sample(\"obs\", dist.Normal(yhat, sigma), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ae835",
   "metadata": {},
   "source": [
    "The next step is to define the `guide` (i.e. variational distribution).\n",
    "In this case, we will try to estimate a typical slope value `slope_loc` (loc refers to \"location\"), and the uncertainty in our estimate `slope_scale`. The variational distribution for the `slope` parameter is a Normal distribution with mean `slope_loc` and standard deviation `slope_scale`. One we have estimates for our **variational parameters** `slope_loc` and `slope_scape`, we have a way of describing which parameter values are more or less likely in light of the obseved data (and prior knowledge).\n",
    "\n",
    "We do the same for the `intercept` parameter.\n",
    "\n",
    "A `guide` is again a regular python function, and it has to take exactly the same parameters as the `model` function (even if they are not used, as in our case). \n",
    "\n",
    "In the `guide` below, we introduce a new primitive: the `pyro.param` function.\n",
    "This makes sure that the required parameters get defined on the right domain (e.g. scales are positive), and we have to specify an initial guess.\n",
    "\n",
    "Once we have defined our variational parameters, we can sample model parameters using the usual `pyro.sample` primitive. As before, this takes a name and a distribution. The names **HAVE** to correspond to the parameters specified in the model, and the distribution is now not the prior, but the variational distributrion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(data):\n",
    "    # variational parameters for slope\n",
    "    slope_loc = pyro.param(\"slope_loc\", torch.tensor(0.0))\n",
    "    slope_scale = pyro.param(\"slope_scale\", torch.tensor(1.0), constraint=dist.constraints.positive)\n",
    "    \n",
    "    # variational parameters for intercept\n",
    "    intercept_loc = pyro.param(\"intercept_loc\", torch.tensor(0.0))\n",
    "    intercept_scale = pyro.param(\"intercept_scale\", torch.tensor(1.0), constraint=dist.constraints.positive)\n",
    "    \n",
    "    # define the variational distributions\n",
    "    pyro.sample(\"slope\", dist.Normal(slope_loc, slope_scale))\n",
    "    pyro.sample(\"intercept\", dist.Normal(intercept_loc, intercept_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239d636",
   "metadata": {},
   "source": [
    "That's it! Now we'll fit the model to our simulated data using SVI.\n",
    "\n",
    "## 3. Fitting the model with SVI\n",
    "\n",
    "SVI means `stochastic variational inference`, which means that we're using a stochastic algorithm to find an \"optimal\" variational distribution. \"optimal\" refers to optimization of the so-called **evidence lower bound (ELBO)**. In Bayesian stats, the **evidence** of a model is a measure of how well the model is supported by the data. In variational inference, we can estimate a lower bound of this evidence (hence the name), which depends on the variational parameters. By iteratively maximizing the ELBO, we get a better supported model. Pyro is built on top of **pyrorch** which uses automatic differentiation for optimization. However, in SVI, one hase to sample random parameter values to approximate the ELBO (and the gradient of the ELBO). This explains the S (Stochastic) in SVI. \n",
    "\n",
    "Note that in variational autoencoders, the ELBO can be calculated using the **reconstruction loss** and the **KL divergence** (see slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ebb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we run this cell multiple times, we need to clear the \"parameter store\" \n",
    "# i.e. reset all parameters to their initial values.\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the SVI object\n",
    "svi = SVI(\n",
    "    model, # the model function defined above\n",
    "    guide, # the guide function defined above\n",
    "    Adam({\"lr\": 0.01}), # the optimizer (here we use Adam)\n",
    "    loss=Trace_ELBO() # the loss function (here we use the ELBO)\n",
    ")\n",
    "\n",
    "# run the SVI for some number of iterations\n",
    "num_iterations = 2000\n",
    "losses = [] # to store the loss at each iteration and check convergence\n",
    "\n",
    "for step in (pbar := tqdm.notebook.trange(num_iterations)):\n",
    "    loss = svi.step(data) # perform a SVI step and get the loss\n",
    "    # this is where the optimization happens\n",
    "    losses.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        pbar.set_description(f\"Step {step} : loss = {loss:0.3f}\")\n",
    "\n",
    "# plot the loss curve\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "ax.plot(losses)\n",
    "ax.set(xlabel='iteration', ylabel='ELBO loss', title='SVI Loss Curve')\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc4c741",
   "metadata": {},
   "source": [
    "We see that the loss (-ELBO) decreases on average, but we also see a lot of stochastic behavior. This relates to a problem that we'll solve below.\n",
    "\n",
    "Let's see what we estimated and how it fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "# scatter the original data\n",
    "ax.scatter(xs.numpy(), ys.numpy(), color='gray', label='data', zorder=0)\n",
    "\n",
    "# get the learned parameters\n",
    "param_store = pyro.get_param_store()\n",
    "\n",
    "slope_est = param_store['slope_loc'].item()\n",
    "intercept_est = param_store['intercept_loc'].item()\n",
    "\n",
    "# plot the learned regression line\n",
    "x_line = np.array([-1, 5])\n",
    "y_line = slope_est * x_line + intercept_est\n",
    "ax.plot(x_line, y_line, color='k', label='learned regression line', zorder=2)\n",
    "\n",
    "# sample from the learned variational distributions to show uncertainty\n",
    "slope_scale = param_store['slope_scale'].item()\n",
    "intercept_scale = param_store['intercept_scale'].item()\n",
    "legend = 'samples from variational distribution'\n",
    "for _ in range(100):\n",
    "    # here we \"manually\" sample from the variational distributions\n",
    "    # to plot possible regression lines\n",
    "    # below we will see a more convenient way to do this...\n",
    "    slope_sample = np.random.normal(slope_est, slope_scale)\n",
    "    intercept_sample = np.random.normal(intercept_est, intercept_scale)\n",
    "    y_sample_line = slope_sample * x_line + intercept_sample\n",
    "    ax.plot(x_line, y_sample_line, color='r', alpha=0.1, label=legend, zorder=1)\n",
    "    legend = None  # only label the first line\n",
    "\n",
    "ax.legend(fontsize='x-small')\n",
    "ax.set(xlabel='x', ylabel='y', title='Learned Linear Regression with Uncertainty')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c4300",
   "metadata": {},
   "source": [
    "We see that the model captures the linear trend very well, but also that the uncertainty in our estimates is quite large. Next, we'll reconsider some choices to see if we can fix this.\n",
    "\n",
    "## 4. Improving the `guide`\n",
    "\n",
    "In our definition of the guide, we made the assumption that the slope and intercept are independent. However, as the x variable is not centered at zero, we can trade off a higher intercept for a lower slope. To include this in our guide, we would have to introduce the joint distribution of the slope and intercept. This can be a bit tedious. Fortunately, Pyro has a number of \"auto guides\". In this case we'll use the `AutoMultivariateNormal` which does exacly what we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bcbb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "from pyro.infer.autoguide import AutoMultivariateNormal\n",
    "\n",
    "auto_guide = AutoMultivariateNormal(model)\n",
    "\n",
    "svi_auto = SVI(model, auto_guide, Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n",
    "num_iterations = 2000\n",
    "losses_auto = []\n",
    "for step in (pbar := tqdm.notebook.trange(num_iterations)):\n",
    "    loss = svi_auto.step(data)\n",
    "    losses_auto.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        pbar.set_description(f\"Step {step} : loss = {loss:0.3f}\")\n",
    "\n",
    "# plot the loss curve\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "ax.plot(losses_auto)\n",
    "ax.set(xlabel='iteration', ylabel='ELBO loss', title='SVI Loss Curve')\n",
    "\n",
    "pass\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf4715",
   "metadata": {},
   "source": [
    "We quite rapidly converge to an optimum, and the stochasticity is much more constrained than in the previous fit.\n",
    "\n",
    "Again, let's look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again plot the regression results\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "# scatter the original data\n",
    "ax.scatter(xs.numpy(), ys.numpy(), color='gray', label='data', zorder=0)\n",
    "\n",
    "# use Predictive to get samples from the posterior predictive distribution\n",
    "\n",
    "from pyro.infer import Predictive\n",
    "predictive = Predictive(model, guide=auto_guide, num_samples=100)\n",
    "samples = predictive(data=data)\n",
    "slope_samples = samples['slope'].numpy()\n",
    "intercept_samples = samples['intercept'].numpy()\n",
    "\n",
    "# plot the learned regression lines from the samples\n",
    "x_line = np.array([-1, 5])\n",
    "for slope_sample, intercept_sample in zip(slope_samples, intercept_samples):\n",
    "    y_line = slope_sample * x_line + intercept_sample\n",
    "    ax.plot(x_line, y_line, color='r', alpha=0.1, zorder=1)\n",
    "\n",
    "ax.legend(fontsize='x-small')\n",
    "ax.set(xlabel='x', ylabel='y', title='Learned Linear Regression with Uncertainty')\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed575c",
   "metadata": {},
   "source": [
    "Compared to the model with \"manual\" guide, the uncertainty in our estimate is much smaller!\n",
    "\n",
    "## 5. Exercise:\n",
    "\n",
    "1. Come up with a simple model and generate some data. For instance, simulate data with a logistic growth model, or using a $\\sin$ function.\n",
    "2. Write a Pyro model function for your model\n",
    "3. Write your own guide or use an appropriate autoguide\n",
    "4. Fit the model with SVI and inspect the results.\n",
    "5. Diagnose any issues and find ways to improve your model/guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96929637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea836f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
